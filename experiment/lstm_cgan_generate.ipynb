{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJW_A4717Pb3"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v88W7A-_7Pb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 13:16:47.141231: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-11 13:16:49.376892: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-11 13:16:49.377007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-11 13:16:49.703115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-11 13:16:50.401839: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-11 13:16:50.403682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-11 13:16:54.004565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjpWGPus7Pb4"
      },
      "source": [
        "## Constants and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item[0]\n",
        "        \n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SBWfI9tK7Pb5"
      },
      "outputs": [],
      "source": [
        "def get_ds():\n",
        "    \"\"\" Get the notes and chords for each midi file in the ./midi_songs directory \n",
        "    emotion = \"Q1\", \"Q2\", \"Q3\", 'Q4' \"\"\"\n",
        "    \n",
        "    songs = []\n",
        "    labels = []\n",
        "\n",
        "    for file in Path(\"/workspaces/Transformer_GANs_music_generation/Dataset/midis_emopia/X_train\").glob(\"*.mid\"):\n",
        "        # Extract the label from the filename\n",
        "        label = file.stem[:2]  \n",
        "\n",
        "        midi = converter.parse(file)\n",
        "        notes_to_parse = midi.flat.notes\n",
        "\n",
        "        song = []\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                song.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        \n",
        "        songs.append(song)\n",
        "        labels.append(label)\n",
        "\n",
        "    return songs, labels\n",
        "\n",
        "def preprocess_data(songs, labels):\n",
        "    \"\"\" Prepare the sequences and labels used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for song in songs for item in song))\n",
        "    # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    # create a dictionary to map labels to integers\n",
        "    label_to_int = dict((label, number) for number, label in enumerate(sorted(set(labels))))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "    network_labels = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs for each song\n",
        "    for song, label in zip(songs, labels):\n",
        "        for i in range(0, len(song) - sequence_length, 1):\n",
        "            sequence_in = song[i:i + sequence_length]\n",
        "            sequence_out = song[i + sequence_length]\n",
        "            network_input.append([note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(note_to_int[sequence_out])\n",
        "            network_labels.append(label_to_int[label])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(len(set(pitchnames)))\n",
        "\n",
        "    network_output = np.array(network_output)\n",
        "    network_output = keras.utils.to_categorical(network_output)\n",
        "\n",
        "    network_labels = np.array(network_labels)\n",
        "\n",
        "    return (network_input, network_output, network_labels)\n",
        "\n",
        "\n",
        "ds = get_ds()\n",
        "ds = preprocess_data(*ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Flatten, Embedding, multiply, concatenate\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "NUM_LABELS = 4 # number of emotion labels\n",
        "SEQ_LEN = 100 # length of music sequence\n",
        "LATENT_DIM = 100 # latent dimension for generator input\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "SAMPLE_INTERVAL = 1\n",
        "\n",
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = str(item)\n",
        "             \n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
        "\n",
        "\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self, rows, choose_emotion_label = lambda: np.random.randint(0, NUM_LABELS)):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 100\n",
        "        self.num_labels = 4\n",
        "        self.choose_emotion_label = choose_emotion_label\n",
        "        self.disc_loss = []\n",
        "        self.gen_loss =[]\n",
        "        \n",
        "        optimizer = tf.keras.optimizers.legacy.Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        seq = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated sequence as input and determines validity\n",
        "        valid = self.discriminator([seq, label])\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains generator to fool discriminator\n",
        "        self.combined = Model([noise, label], valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
        "        model.add(Bidirectional(LSTM(512)))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.summary()\n",
        "\n",
        "        seq = Input(shape=self.seq_shape)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_labels+1, 100)(label))\n",
        "        flat_seq = Flatten()(seq)\n",
        "        model_input = multiply([flat_seq, label_embedding])\n",
        "        validity = model(model_input)\n",
        "\n",
        "        return Model([seq, label], validity)\n",
        "    \n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.seq_shape))\n",
        "        model.summary()\n",
        "        \n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_labels+1, 100)(label))\n",
        "        model_input = multiply([noise, label_embedding])\n",
        "        seq = model(model_input)\n",
        "\n",
        "    \n",
        "        return Model([noise, label], seq)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load and convert the data\n",
        "        X_train = ds[0]\n",
        "        y_train = ds[2]\n",
        "        print(\"X_train shape: \", X_train.shape)\n",
        "        print(\"y_train shape: \", y_train.shape)\n",
        "\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        \n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            seqs = X_train[idx]\n",
        "            labels = y_train[idx]\n",
        "\n",
        "            # Generate a batch of new note sequences\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_seqs = self.generator.predict([noise, labels])\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch([seqs, labels], real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_seqs, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            seqs = X_train[idx]\n",
        "            labels = y_train[idx]\n",
        "\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch([noise, labels], real)\n",
        "\n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            self.disc_loss.append(d_loss[0])\n",
        "            self.gen_loss.append(g_loss)\n",
        "            # If at save interval => save generated sequence samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                chosen_label = self.choose_emotion_label()\n",
        "                print(\"Generating for emotion: \", chosen_label)\n",
        "                self.generate_for_emotion(seqs, chosen_label, 1, True)\n",
        "\n",
        "        self.plot_loss()\n",
        "    def generate_for_emotion(self, input_seqs, input_labels, num_samples=1, train = True):  \n",
        "        \"\"\" Generate a piano midi file based on user chosen emotion and a trained model \"\"\"\n",
        "        if train:\n",
        "           \n",
        "            # Get pitch names and store in a dictionary\n",
        "            input_labels = np.array([input_labels])\n",
        "            notes = input_seqs\n",
        "            flat_notes = [pitch for item in notes for pitch in item]\n",
        "            pitchnames = sorted(set(flat_notes))\n",
        "            int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "            \n",
        "            # Use random noise to generate sequences\n",
        "            noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "            predictions = self.generator.predict([noise, input_labels])\n",
        "            \n",
        "            pred_notes = [x*242+242 for x in predictions[0]]\n",
        "            \n",
        "            # Map generated integer indices to note names, with error handling\n",
        "            pred_notes_mapped = []\n",
        "            for x in pred_notes:\n",
        "                index = int(x)\n",
        "                if index in int_to_note:\n",
        "                    pred_notes_mapped.append(int_to_note[index])\n",
        "                else:\n",
        "                    # Fallback mechanism: Choose a default note when the index is out of range\n",
        "                    pred_notes_mapped.append('C5')\n",
        "            \n",
        "            # create number of notes to be generated\n",
        "            self.num_samples = num_samples\n",
        "            for i in range(self.num_samples):\n",
        "                create_midi(pred_notes_mapped, 'trained_{}gan_output{}'.format(input_labels[0], i))\n",
        "\n",
        "        if train == False:\n",
        "            # Get pitch names and store in a dictionary\n",
        "            input_labels = np.array([input_labels])\n",
        "            notes = input_seqs\n",
        "            flat_notes = [pitch for item in notes for pitch in item]\n",
        "            pitchnames = sorted(set(flat_notes))\n",
        "            int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "            \n",
        "            # use other sequences to generate sequences\n",
        "            noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "            predictions = self.generator.predict([noise, input_labels])\n",
        "            \n",
        "            pred_notes = [x*242+242 for x in predictions[0]]\n",
        "            \n",
        "            # Map generated integer indices to note names, with error handling\n",
        "            pred_notes_mapped = []\n",
        "            for x in pred_notes:\n",
        "                index = int(x)\n",
        "                if index in int_to_note:\n",
        "                    pred_notes_mapped.append(int_to_note[index])\n",
        "                else:\n",
        "                    # Fallback mechanism: Choose a default note when the index is out of range\n",
        "                    pred_notes_mapped.append('C5')\n",
        "            \n",
        "            # create number of notes to be generated\n",
        "            self.num_samples = num_samples\n",
        "            for i in range(self.num_samples):\n",
        "                create_midi(pred_notes_mapped, 'generated_{}gan_output{}'.format(input_labels[0], i))\n",
        "            return pred_notes_mapped\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss, c='red')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('GAN_Loss_per_Epoch_final{}.png'.format(self.choose_emotion_label()))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" Save the model \"\"\"\n",
        "        self.generator.save(\"generator0.h5\")\n",
        "        self.discriminator.save(\"discriminator0.h5\")\n",
        "        self.combined.save(\"combined0.h5\")\n",
        "        self.generator.save_weights(\"generator_weights0.h5\")\n",
        "        self.discriminator.save_weights(\"discriminator_weights0.h5\")\n",
        "        self.combined.save_weights(\"combined_weights0.h5\")\n",
        "        print(\"Saved model to disk\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    gan = GAN(SEQ_LEN, lambda:0)\n",
        "    gan.train(epochs=EPOCHS, batch_size=BATCH_SIZE, sample_interval=SAMPLE_INTERVAL)\n",
        "    gan.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# generate new musics based on new seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/music21/stream/base.py:3694: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
            "  return self.iter().getElementsByClass(classFilterList)\n"
          ]
        }
      ],
      "source": [
        "def get_ds_test():\n",
        "    \"\"\" Get the notes and chords for each midi file in the ./midi_songs directory \n",
        "    emotion = \"Q1\", \"Q2\", \"Q3\", 'Q4' \"\"\"\n",
        "    \n",
        "    songs = []\n",
        "    labels = []\n",
        "\n",
        "    for file in Path(\"/workspaces/Transformer_GANs_music_generation/Dataset/midis_emopia/X_test\").glob(\"*.mid\"):\n",
        "        # Extract the label from the filename\n",
        "        label = file.stem[:2]  \n",
        "\n",
        "        midi = converter.parse(file)\n",
        "        notes_to_parse = midi.flat.notes\n",
        "\n",
        "        song = []\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                song.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        \n",
        "        songs.append(song)\n",
        "        labels.append(label)\n",
        "\n",
        "    return songs, labels\n",
        "\n",
        "def preprocess_data(songs, labels):\n",
        "    \"\"\" Prepare the sequences and labels used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for song in songs for item in song))\n",
        "    # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    # create a dictionary to map labels to integers\n",
        "    label_to_int = dict((label, number) for number, label in enumerate(sorted(set(labels))))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "    network_labels = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs for each song\n",
        "    for song, label in zip(songs, labels):\n",
        "        for i in range(0, len(song) - sequence_length, 1):\n",
        "            sequence_in = song[i:i + sequence_length]\n",
        "            sequence_out = song[i + sequence_length]\n",
        "            network_input.append([note_to_int[char] for char in sequence_in])\n",
        "            network_output.append(note_to_int[sequence_out])\n",
        "            network_labels.append(label_to_int[label])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(len(set(pitchnames)))\n",
        "\n",
        "    network_output = np.array(network_output)\n",
        "    network_output = keras.utils.to_categorical(network_output)\n",
        "\n",
        "    network_labels = np.array(network_labels)\n",
        "\n",
        "    return (network_input, network_output, network_labels)\n",
        "\n",
        "\n",
        "ds_test = get_ds_test()\n",
        "ds_test = preprocess_data(*ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 100, 512)          1052672   \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirect  (None, 1024)              4198400   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " leaky_re_lu_50 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_51 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5907200 (22.53 MB)\n",
            "Trainable params: 5907200 (22.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_62 (Dense)            (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu_52 (LeakyReLU)  (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_30 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_53 (LeakyReLU)  (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_31 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_54 (LeakyReLU)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_32 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 100)               102500    \n",
            "                                                                 \n",
            " reshape_10 (Reshape)        (None, 100, 1)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 792420 (3.02 MB)\n",
            "Trainable params: 788836 (3.01 MB)\n",
            "Non-trainable params: 3584 (14.00 KB)\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 113ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_64951/1203514079.py:247: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  index = int(x)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.21515892420537897,\n",
              " 0.044009779951100246,\n",
              " 0.3080684596577017,\n",
              " 0.1882640586797066,\n",
              " 0.6161369193154034,\n",
              " 0.14425427872860636,\n",
              " 0.31295843520782396,\n",
              " 0.49877750611246946,\n",
              " 0.8410757946210269,\n",
              " 0.9755501222493888,\n",
              " 0.03178484107579462,\n",
              " 'C5',\n",
              " 'C5',\n",
              " 0.9682151589242054,\n",
              " 'C5',\n",
              " 0.02689486552567237,\n",
              " 0.019559902200488997,\n",
              " 0.4449877750611247,\n",
              " 0.726161369193154,\n",
              " 0.07090464547677261,\n",
              " 'C5',\n",
              " 0.2567237163814181,\n",
              " 0.3276283618581907,\n",
              " 0.9951100244498777,\n",
              " 'C5',\n",
              " 0.16625916870415647,\n",
              " 0.07579462102689487,\n",
              " 0.21271393643031786,\n",
              " 0.4743276283618582,\n",
              " 0.8581907090464548,\n",
              " 0.7530562347188264,\n",
              " 0.7677261613691931,\n",
              " 0.13447432762836187,\n",
              " 'C5',\n",
              " 0.15892420537897312,\n",
              " 0.5378973105134475,\n",
              " 'C5',\n",
              " 0.5476772616136919,\n",
              " 0.7628361858190709,\n",
              " 0.823960880195599,\n",
              " 0.1100244498777506,\n",
              " 'C5',\n",
              " 'C5',\n",
              " 0.8581907090464548,\n",
              " 'C5',\n",
              " 0.6136919315403423,\n",
              " 0.43765281173594134,\n",
              " 0.2371638141809291,\n",
              " 'C5',\n",
              " 0.9633251833740831,\n",
              " 0.0,\n",
              " 0.32273838630806845,\n",
              " 0.5232273838630807,\n",
              " 0.034229828850855744,\n",
              " 0.19315403422982885,\n",
              " 0.6210268948655256,\n",
              " 0.02444987775061125,\n",
              " 'C5',\n",
              " 0.9902200488997555,\n",
              " 'C5',\n",
              " 0.9144254278728606,\n",
              " 'C5',\n",
              " 'C5',\n",
              " 0.4572127139364303,\n",
              " 'C5',\n",
              " 0.5574572127139364,\n",
              " 0.30317848410757947,\n",
              " 'C5',\n",
              " 0.04645476772616137,\n",
              " 0.16136919315403422,\n",
              " 0.4621026894865526,\n",
              " 0.4474327628361858,\n",
              " 0.9877750611246944,\n",
              " 0.8581907090464548,\n",
              " 'C5',\n",
              " 'C5',\n",
              " 0.13202933985330073,\n",
              " 'C5',\n",
              " 0.0024449877750611247,\n",
              " 'C5',\n",
              " 0.3080684596577017,\n",
              " 'C5',\n",
              " 0.0024449877750611247,\n",
              " 0.034229828850855744,\n",
              " 'C5',\n",
              " 0.061124694376528114,\n",
              " 0.9755501222493888,\n",
              " 0.37163814180929094,\n",
              " 0.8875305623471883,\n",
              " 0.7555012224938875,\n",
              " 0.3960880195599022,\n",
              " 0.7359413202933985,\n",
              " 0.061124694376528114,\n",
              " 0.7359413202933985,\n",
              " 0.589242053789731,\n",
              " 'C5',\n",
              " 'C5',\n",
              " 'C5',\n",
              " 0.05623471882640587,\n",
              " 0.5965770171149144]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use trained model to generate music\n",
        "gan = GAN(SEQ_LEN, lambda:3)\n",
        "gan.generator.load_weights(\"generator_weights3.h5\")\n",
        "gan.generate_for_emotion(ds_test[0], 3, 50, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hear some samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div id=\"midiPlayerDiv2944690\"></div>\n",
              "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
              "        \n",
              "        <script\n",
              "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
              "        ></script>\n",
              "    \n",
              "        <script>\n",
              "        function midiPlayerDiv2944690_play() {\n",
              "            const rq = require.config({\n",
              "                paths: {\n",
              "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
              "                }\n",
              "            });\n",
              "            rq(['music21'], function(music21) {\n",
              "                mp = new music21.miditools.MidiPlayer();\n",
              "                mp.addPlayer(\"#midiPlayerDiv2944690\");\n",
              "                mp.base64Load(\"data:audio/midi;base64,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\");\n",
              "            });\n",
              "        }\n",
              "        if (typeof require === 'undefined') {\n",
              "            setTimeout(midiPlayerDiv2944690_play, 2000);\n",
              "        } else {\n",
              "            midiPlayerDiv2944690_play();\n",
              "        }\n",
              "        </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "file = converter.parse('/workspaces/Transformer_GANs_music_generation/generated_0gan_output46.mid')\n",
        "file.show('midi')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "conditional_gan",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
