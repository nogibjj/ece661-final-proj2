{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-06 20:13:58--  https://raw.githubusercontent.com/asigalov61/tegridy-tools/main/tegridy-tools/GPT2RGA.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38843 (38K) [text/plain]\n",
      "Saving to: ‘GPT2RGA.py.1’\n",
      "\n",
      "GPT2RGA.py.1        100%[===================>]  37.93K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2023-12-06 20:13:58 (19.6 MB/s) - ‘GPT2RGA.py.1’ saved [38843/38843]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/asigalov61/tegridy-tools/main/tegridy-tools/GPT2RGA.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import secrets\n",
    "import copy\n",
    "import tqdm\n",
    "from tqdm import auto\n",
    "\n",
    "from GPT2RGA import *\n",
    "\n",
    "from miditok import REMI, CPWord\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a multitrack tokenizer configuration, read the doc to explore other parameters\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Loads a midi, converts to tokens, and back to a MIDI\n",
    "midi = MidiFile('midis/Q1__8v0MFBZoco_0.mid')\n",
    "tokens = tokenizer(midi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDIs (token):   0%|          | 0/1071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDIs (token): 100%|██████████| 1071/1071 [00:34<00:00, 31.46it/s]\n",
      "Performing data augmentation: 100%|██████████| 1071/1071 [00:05<00:00, 179.87it/s]\n",
      "Loading token files: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a whole dataset and save it at Json files\n",
    "midi_paths = list(Path(\"midis/\").glob(\"*.mid\"))\n",
    "data_augmentation_offsets = [2, 1, 1]  # data augmentation on 2 pitch octaves, 1 velocity and 1 duration values\n",
    "tokenizer.tokenize_midi_dataset(midi_paths, Path(\"token\"),\n",
    "                                data_augment_offsets=data_augmentation_offsets)\n",
    "\n",
    "# Constructs the vocabulary with BPE, from the token files\n",
    "tokenizer.learn_bpe(\n",
    "    vocab_size=10000,\n",
    "    tokens_paths=list(Path(\"path\", \"to\", \"tokens_noBPE\").glob(\"**/*.json\")),\n",
    "    start_from_empty_voc=False,\n",
    ")\n",
    "\n",
    "# Saving our tokenizer, to retrieve it back later with the load_params method\n",
    "tokenizer.save_params(Path(\"token/tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token/tokenizer.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 6.72kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 27.7MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 300kB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:01<00:00, 237MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = data[\"text_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tensor = tokenizer.encode(tokens, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
